{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Assignment 1 - Q1, Q2, Q3\n",
    "\n",
    "Amaya Syed - 190805496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from random import shuffle\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this assignment is to classify a corpus of 21 000 Amazon reviews, equally distributed between \"real\" or \"fake\" reviews. where fake is defined as being reviews written for a product with the intent of receiving financial compensation for doing so. The reviews have been previously analysed and manually labelled by the company itself. The corpus also contains information regarding each reviews: rating, verified purchase, product category, product ID, product title, review title.\n",
    "\n",
    "This task is therefore fundamentally one of deception detection and detection by an NLP algorithm of the stylistc differences that might arise between spontaneous reviews left by consummers and reviews that are written motivated by a potential monetary gain.\n",
    "\n",
    "We will first implement the functions detailed in Question 1, 2, and 3 of this assignement and then build on them to try better our classification accuracy by using the features provided with the reviews and which might potentially give us better insight into the differences between real and fake reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very first task for an NLP task is to preprocess the input text in such a way as to make it usable for the machine learning algorithm of our choice. To do so one must first load the data (*cf.* **loadData** function), in this case a tab delimitated text file, with utf8 coding. The data is loaded review by review, with all the features associated to said review kept in a list. The **parseReview** function then accepts as input each review (also indicated as *line*) and keep only the review ID as an integer, labels as real or fake and review text as string, passes the label names to real or fake, as well as passing the review ID from string to integer. Each element of the list is then assigned to a separate variable: *Id*, *Text* and *Label* and returned by **parseReview** within **loadData**. Each triple so returned is then appended to the *rawData* variable and kept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1:\n",
    "\n",
    "# a) Convert line from input file into an id/text/label tuple\n",
    "\n",
    "\n",
    "def str_to_int(element):\n",
    "    \n",
    "    \"\"\"\n",
    "    Auxiliary function which takes an element of a list and tries to pass it from string to integer. If it can't it returns the original element.'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        element = int(element) # trying to convert string to integer\n",
    "    except ValueError: # if Value Error is raised because the string can't be passed to integer, then pass and return original input\n",
    "        pass\n",
    "    return element\n",
    "\n",
    "\n",
    "def parseReview(line):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a triple of an integer, a string containing the review, and a string indicating the label.\n",
    "  \n",
    "    \"\"\"\n",
    "    indices = [0, 1, 8] # create a list of indices to keep in the review list\n",
    "    label_dict = {'__label1__':'fake', '__label2__':'real'} # creating a dictionary assigning labels to more descriptive terms. \n",
    "            \n",
    "    review = [line[index] for index in indices] # 2) keep id, label, text using the indices\n",
    "    review = [label_dict.get(x, x) for x in review] # 3) passing the label names to real or fake\n",
    "    review = [str_to_int(x) for x in review] # 4) pass id to int using the conv function defined above\n",
    "    Id, Label, Text = review # 5) assign each element of the list to a separate variable\n",
    "        \n",
    "    return Id, Text, Label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to tokenise the text review, which we will do in the **preProcess** function, which accepts as input an element of the *Text* list of *rawData* and is called from inside the **spliData** function. First space is added between words and punctionion, then the text is split using white space as the divider and tokens are all passed to lower case and returned as list of strings, where each token is an element. No other preprocessing, such as lemmatization, stop-word removal or punctuation removal was attempted at this stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "# b) TEXT PREPROCESSING AND FEATURE VECTORIZATION        \n",
    "\n",
    "# Input: a string of one  review\n",
    "def preProcess(text):\n",
    "    \n",
    "    order = 1\n",
    "    # Simple tokenisation\n",
    "    text = re.sub(r\"(\\w)([.,;:!?'\\\"\\)])\", r\"\\1 \\2\", text) # add a space between word and the punctuation in between square bracket\n",
    "    text = re.sub(r\"([.,;:!?'\\\"\\(])(\\w)\", r\"\\1 \\2\", text) # add a space between punctuation and word\n",
    "     \n",
    "    tokens = re.split(r\"\\s+\", text) # divide the text into tokens using white space as the divider\n",
    "    tokens = [t.lower() for t in tokens] # pass all words to lower case\n",
    "    #tokens = ['<s>'] * (order-1) + tokens + ['</s>'] # add beginning to pad for order > 2 and ending sequence for each review. \n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenised text of a review can now be passed as input to the **toFeatureVector** function, which initialises a global dictionary (*featureDict*) and a local dictionary (*token_frequency*) of tokens present in the reviews. We then go over all the elements in the token list and check if they're in the local and global dictionary: if they aren't we add them as dictionary keys with a frequency of one, whilst if they are, we just add one to that count. We therefore obtain a local dictionary with the tokens counts for a given review and a global dictionary with the token counts for all reviews. We return the local token frequency dictionary within **splitData**, which associates it to its given label and splits the data into a training and testing sets using the lenght of *rawData* and a chosen percentage input by the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDict = {} # A global dictionary of features\n",
    "\n",
    "def toFeatureVector(tokens):\n",
    "    \"\"    \n",
    "    \"\"\n",
    "    token_frequency = {} # creating a local dictionary for token frequency in the review \n",
    "\n",
    "    for token in tokens: # for each word in the review\n",
    "    \n",
    "        if token not in token_frequency.keys(): \n",
    "            token_frequency[token] = 1  # if the word is not within the dict we add it as key = 1\n",
    "        else:\n",
    "            token_frequency[token] += 1  # if its already in the dic, add one\n",
    "            \n",
    "        if token not in featureDict.keys(): # same for global dictionary\n",
    "             featureDict[token] = 1\n",
    "        else:\n",
    "            featureDict[token] += 1\n",
    "        \n",
    "    return token_frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "\n",
    "def loadData(path, Text=None):\n",
    "    \n",
    "    with open(path, encoding='utf8') as f: # open file\n",
    "        \n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        next(reader)\n",
    "        \n",
    "        for line in reader: # each line corresponds to a review and its associated features\n",
    "            \n",
    "            (Id, Text, Label) = parseReview(line) # keep the id, text, label triple for a review\n",
    "            \n",
    "            rawData.append((Id, Text, Label)) # keep the triple for all reviews\n",
    "            \n",
    "\n",
    "\n",
    "def splitData(percentage):\n",
    "    \n",
    "    # A method to split the data between trainData and testData \n",
    "    \n",
    "    dataSamples = len(rawData) #lenght of the dataset\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2) # size of training set based on percentage chosen by user. \n",
    "    \n",
    "    for (_, Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Text)),Label))\n",
    "        \n",
    "    for (_, Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Text)),Label))           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(max_iter=3500))])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is organised as a dictionary of token frequencies for each review, we can use it to train our Support Vector Machine (SVM) classifier on the training set. We must first define the **crossValidate** function which will take our training data and successively split it into a training and a hold out set. The size of the hold out set will depend on the number of folds we wish to divide our data into and is a parameter of the function. The cross validation will then loop over the data creating a foldsize split for the test data, which moves across the data length and a training set composed of all the data except the hold out set. The classifier is training on the training set and then its predictive abilities tested on the hold out set - measuring precision, recall and fscore. We therefore retrain our classifier for each different split and obtain different accuracy results on the testing set: the accuracy results are then presented as the average of the results obtained in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidate(trainData, folds):\n",
    "\n",
    "    shuffle(trainData) # reshuffle dataset as currently its ordered into fake, then real reviews\n",
    "    cv_results = [] # initiate list to keep cv results for each fold\n",
    "    foldSize = int(len(trainData) / folds) # initiate fold size by dividing training data by # folds.\n",
    "    \n",
    "    for i in range(0, len(trainData), foldSize): # data goes from i to lenght of data in foldsize jumps.\n",
    "        \n",
    "        cv_test = trainData[i : i + foldSize] # from i to i + foldsize -> for 1st loop test data will be 0 to i_foldsize, 2nd loop from i_foldize to i_(foldsize*2), therefore going over the whole training data in foldsize chunks. \n",
    "        cv_train  = trainData[0 : i] + trainData[i + foldSize : ] # for 1st loop from 0 to 0 + i_foldsize, 2nd loop from 0 to i_foldsize + i_(foldsize*2) to end, 3rd loop 0 to i_(foldsize*2) + i_(foldsize*3) to end, etc\n",
    "        \n",
    "        classifier = trainClassifier(cv_train)  # train the SVM classifier\n",
    "        testTrue = [t[1] for t in cv_test]   # get the ground-truth labels from the data\n",
    "        testPred = predictLabels(cv_test, classifier)  # classify the test data to get predicted labels\n",
    "        precision, recall, fscore, _ = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "        cv_results.append((precision, recall, fscore)) # append results obtained for each training set \n",
    "    cv_results = (np.mean(np.array(cv_results), axis = 0)) # average the cv results for precision, recall and fscore\n",
    "   # print(cv_results)\n",
    "    print(\"Done training!\")\n",
    "    print(\"Precision %f\\nRecall: %f\\nF Score:%f\" % (cv_results[0], cv_results[:1], cv_results[2]))   \n",
    "    \n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAIN part of the code is below and calls all the functions defined above to train the SVM classifier using cross validation to check the consistency of our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 21000 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "After split, 21000 rawData, 16800 trainData, 4200 testData\n",
      "Training Samples: \n",
      "16800\n",
      "Features: \n",
      "43900\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Done training!\n",
      "Precision 0.618166\n",
      "Recall: 0.618166\n",
      "F Score:0.617615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.61816613, 0.6177381 , 0.61761458])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "folds = 10\n",
    "# loading reviews\n",
    "# initialize global lists that will be appended to by the methods below\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "trainData = []        # the pre-processed training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the pre-processed test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "# the output classes\n",
    "fakeLabel = 'fake'\n",
    "realLabel = 'real'\n",
    "\n",
    "# references to the data files\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "# Do the actual stuff (i.e. call the functions we've made)\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "loadData(reviewPath) \n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# You do the cross validation on the 80% (training data)\n",
    "# We print the number of training samples and the number of features before the split\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "\n",
    "splitData(0.8)\n",
    "\n",
    "# We print the number of training samples and the number of features after the split\n",
    "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')\n",
    "\n",
    "# QUESTION 3 - Make sure there is a function call here to the\n",
    "# crossValidate function on the training set to get your results\n",
    "\n",
    "crossValidate(trainData, folds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain results which correspond to a 10 % increase above that expected by always naively predicting the most frequent outcome, as we know there is no class imbalance between labels and the minimum accuracy obtained should be 50%. There is therefore still a significant amount of work to be done to obtain higher accuracy results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have checked our accuracy results are robust by using cross-validation and that the functions defined all work and intermesh as they should, we will train our classifier on all the *trainData* available and test in on the *testData* obtained from **splitData**. We present the results in terms of precision, recall and fscore, as above. We broadly obtain similar results to the ones obtained by cross-validation, although they are one point lower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'this': 2, 'assortment': 1, 'is': 1, 'really': 1, 'hershey': 1, \"'\": 1, 's': 1, 'at': 1, 'their': 1, 'best': 1, '.': 2, 'the': 2, 'little': 1, 'ones': 1, 'are': 1, 'always': 1, 'excited': 1, 'whenever': 1, 'holidays': 1, 'come': 1, 'because': 1, 'of': 1}, 'fake')\n",
      "Training Classifier...\n",
      "Done training!\n",
      "Precision: 0.603607\n",
      "Recall: 0.603571\n",
      "F Score:0.603537\n"
     ]
    }
   ],
   "source": [
    "# Finally, check the accuracy of your classifier by training on all the tranin data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(testData[0])   # have a look at the first test data instance\n",
    "    classifier = trainClassifier(trainData)  # train the classifier\n",
    "    testTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
    "    testPred = predictLabels(testData, classifier)  #Â classify the test data to get predicted labels\n",
    "    finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "    print(\"Done training!\")\n",
    "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % finalScores[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
