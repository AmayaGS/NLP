{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Assignment 1 - Q4\n",
    "\n",
    "Amaya Syed - 190805496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have established a low benchmark with which to compare accuracy results, we can start trying to incorporate different elements into the function defined in part 1 to study their effect on overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_int(element):\n",
    "    \n",
    "    \"\"\"\n",
    "    Auxiliary function which takes an element of a list and tries to pass it from string to integer. If it can't it returns the original element.'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        element = int(element) # trying to convert string to integer\n",
    "    except ValueError: # if Value Error is raised because the string can't be passed to integer, then pass and return original input\n",
    "        pass\n",
    "    return element\n",
    "\n",
    "\n",
    "def parseReview(line):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a triple of an integer, a string containing the review, and a string indicating the label.\n",
    "  \n",
    "    \"\"\"\n",
    "    indices = [0, 1, 8] # create a list of indices to keep in the review list\n",
    "    label_dict = {'__label1__':'fake', '__label2__':'real'} # creating a dictionary assigning labels to more descriptive terms. \n",
    "            \n",
    "    review = [line[index] for index in indices] # 2) keep id, label, text using the indices\n",
    "    review = [label_dict.get(x, x) for x in review] # 3) passing the label names to real or fake\n",
    "    review = [str_to_int(x) for x in review] # 4) pass id to int using the conv function defined above\n",
    "    Id, Label, Text = review # 5) assign each element of the list to a separate variable\n",
    "        \n",
    "    return Id, Text, Label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we can do to improve the accuracy is to improve the preprocessing. As mentioned earlier, possible strategies for this are lemmatization, stop-word removal or punctuation removal. We implement all three of these strategies:\n",
    "\n",
    "- lemmatisation is the technique whereby we convert a word to its base form, therefore seeking to improve the sparseness of the word count. We implemenet lemmatisation by using the WordNetLemmatizer package from NLTK and downloading the known lemmatisations of English words. \n",
    "\n",
    "- stop word removel consists in removing connective words from the text, therefore removing frequent words with unimportant meaning. This also improves sparseness. \n",
    "\n",
    "- Removing punctuation, which as it says, is simply stripping the text of any punctuation marks - improving sparseness and removing potentially unimportant information.\n",
    "\n",
    "A caveat to the above descriptions is that because the task is fake review detection, removing this information might actually hurt our classification as small stylistic differences might be the main differences between a fake and a real review, at least if one is solely looking at the review text itself.\n",
    "\n",
    "In addition to this we can also add bigram or even trigram tokens to our unigram tokens, which might help detect patterns of speech between reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) TEXT PREPROCESSING AND FEATURE VECTORIZATION        \n",
    "\n",
    "# Input: a string of one  review\n",
    "def preProcess(text):\n",
    "    \n",
    "    # we can try lemmatizing the words and check effect on accuracy - lemmatizing converts a word to its base form.\n",
    "    \n",
    "    # Initialting the Wordnet Lemmatizer\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokens = []\n",
    "    output = []  \n",
    "    \n",
    "    #stop_words = set(stopwords.words('english')) # removing stop words actually decreases accuracy slightly\n",
    "    \n",
    "    for token in text.split():\n",
    "        \n",
    "        #if token not in stop_words: # stop word removal\n",
    "        \n",
    "        #tokens.append(lemmatizer.lemmatize(token.lower())) # lemmatization of the tokens\n",
    "        tokens.append(token.lower())\n",
    "        \n",
    "        output = [' '.join(token) for token in nltk.bigrams(tokens)] + tokens\n",
    "        # adding trigrams doesn't do anything for the accuracy. Using bigrams and unigram adds a marginal increment in accuracy, but is slower to run. \n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our **toFeatureVector** function we have implemented a simple count of token frequency for each review. To continue making this function more sophisticated, it would be necessary to implement a different weighting scheme, such as term frequency–inverse document frequency, amongst other potential schemes. However, in this case, due to lack of programming knowledge it was not possible to implement any of these schemes in such a way that it made sense and didn't take an extremely long time to compute. We therefore made the decision to continue forward using count data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDict = {}\n",
    "\n",
    "def toFeatureVector(tokens):\n",
    "    \n",
    "    token_frequency = {} # creating a local dictionary for token frequency in the review\n",
    "    \n",
    "    for token in tokens: # for each word in the review\n",
    "    \n",
    "        if token not in token_frequency.keys(): \n",
    "            token_frequency[token] = 1  # if the word is not within the dict we add it as key = 1\n",
    "        else:\n",
    "            token_frequency[token] += 1  # if its already in the dic, add one\n",
    "    \n",
    "        if token not in featureDict.keys():\n",
    "            featureDict[token] = 1\n",
    "        else:\n",
    "            featureDict[token] += 1\n",
    "                \n",
    "    return token_frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "\n",
    "\n",
    "def loadData(path, Text=None):\n",
    "    \n",
    "    with open(path, encoding='utf8') as f: # open file\n",
    "        \n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        next(reader)\n",
    "        \n",
    "        for line in reader: # each line corresponds to a review and its associated features\n",
    "            \n",
    "            (Id, Text, Label) = parseReview(line) # keep the id, text, label triple for a review\n",
    "            \n",
    "            rawData.append((Id, Text, Label)) # keep the triple for all reviews\n",
    "            \n",
    "\n",
    "def splitData(percentage):\n",
    "    \n",
    "    # A method to split the data between trainData and testData \n",
    "    \n",
    "    dataSamples = len(rawData) #lenght of the dataset\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2) # size of training set based on percentage chosen by user. \n",
    "    \n",
    "    for (_, Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Text)),Label))\n",
    "        \n",
    "    for (_, Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Text)),Label))           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue tweaking the accuracy we can change some parameter values of the SVM classifier. In our case, after running the classifier with different values of *C* we settled on a value of **0.001**. The *C* parameter essentially acts a regularisation parameter which controls the tradeoff between minimising training error and minimsing the norm of the weights, where a small value of *C* means more regularisation (smaller weights). In practical terms it means the hyperplane dividing the data will have a larger minimum marginnif the value of C is smaller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(max_iter=3000, C = 0.001))])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidate(trainData, folds):\n",
    "\n",
    "    shuffle(trainData) # reshuffle dataset as currently its ordered into fake, then real reviews\n",
    "    cv_results = [] # initiate list to keep cv results for each fold\n",
    "    foldSize = int(len(trainData) / folds) # initiate fold size by dividing training data by # folds.\n",
    "    \n",
    "    for i in range(0, len(trainData), foldSize): # data goes from i to lenght of data in foldsize jumps.\n",
    "        \n",
    "        cv_test = trainData[i : i + foldSize] # from i to i + foldsize -> for 1st loop test data will be 0 to i_foldsize, 2nd loop from i_foldize to i_(foldsize*2), therefore going over the whole training data in foldsize chunks. \n",
    "        cv_train  = trainData[0 : i] + trainData[i + foldSize : ] # for 1st loop from 0 to 0 + i_foldsize, 2nd loop from 0 to i_foldsize + i_(foldsize*2) to end, 3rd loop 0 to i_(foldsize*2) + i_(foldsize*3) to end, etc\n",
    "        \n",
    "        classifier = trainClassifier(cv_train)  # train the SVM classifier\n",
    "        testTrue = [t[1] for t in cv_test]   # get the ground-truth labels from the data\n",
    "        testPred = predictLabels(cv_test, classifier)  # classify the test data to get predicted labels\n",
    "        precision, recall, fscore, _ = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "        cv_results.append((precision, recall, fscore)) # append results obtained for each training set \n",
    "    cv_results = (np.mean(np.array(cv_results), axis = 0)) # average the cv results for precision, recall and fscore\n",
    "   # print(cv_results)\n",
    "    print(\"Done training!\")\n",
    "    print(\"Precision %f\\nRecall: %f\\nF Score:%f\" % (cv_results[0], cv_results[:1], cv_results[2]))   \n",
    "    \n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAIN part of the code is below and calls all the functions defined above to train the SVM classifier using cross validation to check the consistency of our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 21000 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "After split, 21000 rawData, 16800 trainData, 4200 testData\n",
      "Training Samples: \n",
      "16800\n",
      "Features: \n",
      "592379\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Done training!\n",
      "Precision 0.652254\n",
      "Recall: 0.652254\n",
      "F Score:0.650863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.6522536 , 0.65113095, 0.6508634 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "folds = 10\n",
    "# loading reviews\n",
    "# initialize global lists that will be appended to by the methods below\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "trainData = []        # the pre-processed training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the pre-processed test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "# the output classes\n",
    "fakeLabel = 'fake'\n",
    "realLabel = 'real'\n",
    "\n",
    "# references to the data files\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "# Do the actual stuff (i.e. call the functions we've made)\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "loadData(reviewPath) \n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# You do the cross validation on the 80% (training data)\n",
    "# We print the number of training samples and the number of features before the split\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "\n",
    "splitData(0.8)\n",
    "\n",
    "# We print the number of training samples and the number of features after the split\n",
    "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')\n",
    "\n",
    "# QUESTION 3 - Make sure there is a function call here to the\n",
    "# crossValidate function on the training set to get your results\n",
    "\n",
    "crossValidate(trainData, folds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain results which correspond to a 10 % increase above that expected by always naively predicting the most frequent outcome, as we know there is no class imbalance between labels and the minimum accuracy obtained should be 50%. There is therefore still a significant amount of work to be done to obtain higher accuracy results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have checked our accuracy results are robust by using cross-validation and that the functions defined all work and intermesh as they should, we will train our classifier on all the *trainData* available and test in on the *testData* obtained from **splitData**. We present the results in terms of precision, recall and fscore, as above. We broadly obtain similar results to the ones obtained by cross-validation, although they are one point lower. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1- Testing on changes in preprocessing**\n",
    "\n",
    "First we check the effect on accuracy for changes in the preprocessing, namely lemmatization, stop word removal and adding bigrams \n",
    "\n",
    "- Lemmatization + unigrams: \n",
    "\n",
    "\n",
    "    - Precision: 0.588811\n",
    "    - Recall: 0.588810\n",
    "    - F Score:0.588808\n",
    "    \n",
    "- Lemmatization + stop words + unigrams:  \n",
    "\n",
    "\n",
    "    - Precision: 0.577639\n",
    "    - Recall: 0.577619\n",
    "    - F Score:0.577591\n",
    "\n",
    "- Lemmatization + bigrams + unigrams:\n",
    "\n",
    "\n",
    "    - Precision: 0.603337\n",
    "    - Recall: 0.603333\n",
    "    - F Score:0.603330\n",
    "\n",
    "- Lemmatization + stopwords + bigrams + unigrams:\n",
    "\n",
    "\n",
    "    - Precision: 0.605477\n",
    "    - Recall: 0.605476\n",
    "    - F Score:0.605475\n",
    "\n",
    "The changes in preprocessing have not affected the accuracy in any significant way. Going forward we will use bigrams and unigrams, but no lemmatization or stop word removal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocessing: just bigrams and unigrams - SVC with C = 0.001\n",
    "\n",
    "\n",
    "    - Precision: 0.628835\n",
    "    - Recall: 0.628333\n",
    "    - F Score:0.627971"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'this assortment': 1, 'assortment is': 1, 'is really': 1, \"really hershey's\": 1, \"hershey's at\": 1, 'at their': 1, 'their best.': 1, 'best. the': 1, 'the little': 1, 'little ones': 1, 'ones are': 1, 'are always': 1, 'always excited': 1, 'excited whenever': 1, 'whenever the': 1, 'the holidays': 1, 'holidays come': 1, 'come because': 1, 'because of': 1, 'of this.': 1, 'this': 1, 'assortment': 1, 'is': 1, 'really': 1, \"hershey's\": 1, 'at': 1, 'their': 1, 'best.': 1, 'the': 2, 'little': 1, 'ones': 1, 'are': 1, 'always': 1, 'excited': 1, 'whenever': 1, 'holidays': 1, 'come': 1, 'because': 1, 'of': 1, 'this.': 1}, 'fake')\n",
      "Training Classifier...\n",
      "Done training!\n",
      "Precision: 0.628835\n",
      "Recall: 0.628333\n",
      "F Score:0.627971\n"
     ]
    }
   ],
   "source": [
    "# Finally, check the accuracy of your classifier by training on all the tranin data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(testData[0])   # have a look at the first test data instance\n",
    "    classifier = trainClassifier(trainData)  # train the classifier\n",
    "    testTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
    "    testPred = predictLabels(testData, classifier)  # classify the test data to get predicted labels\n",
    "    finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "    print(\"Done training!\")\n",
    "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % finalScores[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 815,
   "position": {
    "height": "40px",
    "left": "1159.67px",
    "right": "20px",
    "top": "-9px",
    "width": "517px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
