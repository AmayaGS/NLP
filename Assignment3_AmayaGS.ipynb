{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Classifying Gender of EastEnder Characters based on dialogue snippets\n",
    "# Amaya Syed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Amaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "from random import shuffle\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "named_entity = en_core_web_sm.load()\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this assignment is to correctly classify the gender of characters from the series Eastenders based on dialogue snippets provided by the BBC.\n",
    "\n",
    "There are two files avaibles, a training set file with 10113 dialogue instances and a testing set file with 1124 instances. Ine each the data is arranged in three columns, as such:\n",
    "\n",
    "\" 'Someone had fun.', 'SEAN', 'male' \"\n",
    "\" 'It's no problem, honestly. Go on, go and open the launderette.  Leave it with me.', 'SHIRLEY', 'female' \"\n",
    "\n",
    "Leaving out the name of the character and we will endeavour to classify gender correctly by pinpointing properties of the dialogue that might be relevant. \n",
    "\n",
    "First we load the two files and keep them in the variables **training_set** and **testing_set**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "\n",
    "def loadData(path, Text=None):\n",
    "    \n",
    "    rawData = []\n",
    "    \n",
    "    with open(path, encoding='utf8') as f: # open file\n",
    "        \n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        \n",
    "        for line in reader: # each line corresponds to a review and its associated features\n",
    "            \n",
    "            speech, character, gender = line \n",
    "            \n",
    "            rawData.append((speech, character, gender)) # keep the triple for all reviews\n",
    "            \n",
    "        return rawData\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = loadData('training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Someone had fun.', 'SEAN', 'male')\n"
     ]
    }
   ],
   "source": [
    "# checking first line training set.\n",
    "print(training_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set = loadData('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Kicked you out?', 'STACEY', 'female')\n"
     ]
    }
   ],
   "source": [
    "# # checking first line testing set.\n",
    "print(testing_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now check if the data is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Someone had fun.</td>\n",
       "      <td>SEAN</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's no problem, honestly. Go on, go and open ...</td>\n",
       "      <td>SHIRLEY</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Last night was better than ever. What's all th...</td>\n",
       "      <td>MAX</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Have you checked the answerphone?  Any calls?</td>\n",
       "      <td>IAN</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oscar's asleep.</td>\n",
       "      <td>MAX</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1       2\n",
       "0                                   Someone had fun.     SEAN    male\n",
       "1  It's no problem, honestly. Go on, go and open ...  SHIRLEY  female\n",
       "2  Last night was better than ever. What's all th...      MAX    male\n",
       "3      Have you checked the answerphone?  Any calls?      IAN    male\n",
       "4                                    Oscar's asleep.      MAX    male"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"training.csv\", delimiter = \",\", header = None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_female = data[data[2] == 'female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5032\n"
     ]
    }
   ],
   "source": [
    "print(len(data_female))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_male = data[data[2] == 'male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5081\n"
     ]
    }
   ],
   "source": [
    "print(len(data_male))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of female/male dialogue instance is roughly equal, meaning that the expected accuracy of a non-trained classifier should be 50.5%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also quickly check if male/females have similar average word counts and average word lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.260930325414254\n"
     ]
    }
   ],
   "source": [
    "print((data_female[0].str.count(' ') + 1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.691623864085342\n"
     ]
    }
   ],
   "source": [
    "print((data_male[0].str.count(' ') + 1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.9580754641645\n"
     ]
    }
   ],
   "source": [
    "print(data_female[0].str.len().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.36566574476492\n"
     ]
    }
   ],
   "source": [
    "print(data_male[0].str.len().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are similar, males with slightly higher (4/5 %) average word count and length of word, but which doesnÂ´t seem like it would be significative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline we will run the data through a simple pipeline, with minimal preprocessing and feature engineering, using a simple linear SVM model. We will split the training set into a 80/20 training and dev set. We won't be running cross-validation because the dataset is large enough it doesn't seem necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) TEXT PREPROCESSING AND FEATURE VECTORIZATION        \n",
    "\n",
    "# Input: a string of one  review\n",
    "def preProcess(text):\n",
    "    \n",
    "    order = 1\n",
    "    # Simple tokenisation\n",
    "    text = re.sub(r\"(\\w)([.,;:!?'\\\"\\)])\", r\"\\1 \\2\", text) # add a space between word and the punctuation in between square bracket\n",
    "    text = re.sub(r\"([.,;:!?'\\\"\\(])(\\w)\", r\"\\1 \\2\", text) # add a space between punctuation and word\n",
    "     \n",
    "    tokens = re.split(r\"\\s+\", text) # divide the text into tokens using white space as the divider\n",
    "    tokens = [t.lower() for t in tokens] # pass all words to lower case\n",
    "    #tokens = ['<s>'] * (order-1) + tokens + ['</s>'] # add beginning to pad for order > 2 and ending sequence for each review. \n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'this', 'is', 'the', ',', 'ehh', '...', 'presumably', ',', 'a', 'crying', 'situations', '!']\n"
     ]
    }
   ],
   "source": [
    "print(preProcess(\"hello this is the, ehh... presumably, a crying situations!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDict = {} # A global dictionary of features\n",
    "\n",
    "def toFeatureVector(tokens):\n",
    "    \"\"    \n",
    "    \"\"\n",
    "    token_frequency = {} # creating a local dictionary for token frequency in the review \n",
    "\n",
    "    for token in tokens: # for each word in the review\n",
    "    \n",
    "        if token not in token_frequency.keys(): \n",
    "            token_frequency[token] = 1  # if the word is not within the dict we add it as key = 1\n",
    "        else:\n",
    "            token_frequency[token] += 1  # if its already in the dic, add one\n",
    "            \n",
    "        if token not in featureDict.keys(): # same for global dictionary\n",
    "             featureDict[token] = 1\n",
    "        else:\n",
    "            featureDict[token] += 1\n",
    "        \n",
    "    return token_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(training_set, percentage):\n",
    "    \n",
    "    # A method to split the data between trainData and testData \n",
    "    \n",
    "    dataSamples = len(training_set) #lenght of the dataset\n",
    "    halfOfData = int(len(training_set)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2) # size of training set based on percentage chosen by user. \n",
    "    \n",
    "    for (speech, _, gender) in training_set[:trainingSamples] + training_set[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(speech)), gender))\n",
    "        \n",
    "    for (speech, _, gender) in training_set[trainingSamples:halfOfData] + training_set[halfOfData+trainingSamples:]:\n",
    "        devData.append((toFeatureVector(preProcess(speech)), gender))\n",
    "        \n",
    "    return trainData, devData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([ ('svc', LinearSVC(C = 0.1, max_iter=6000))])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "\n",
    "trainData = []    \n",
    "devData = [] \n",
    "\n",
    "trainData, devData = splitData(training_set, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'alright': 1, '.': 2, 'i': 1, \"'\": 1, 'll': 1, 'try': 1, 'and': 1, 'get': 1, 'the': 1, 'time': 1, 'off': 1, 'work': 1}, {'what': 1, 'are': 1, 'you': 1, 'on': 1, 'about': 1, '?': 1}, {'you': 1, 'didn': 1, \"'\": 1, 't': 1, 'need': 1, 'to': 1, 'wait': 1, '.': 1}]\n"
     ]
    }
   ],
   "source": [
    "# double checking I'm not using the gender label here. \n",
    "print(list(map(lambda t: t[0], devData[0:3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Someone had fun.', 'SEAN', 'male') ({'it': 2, \"'\": 1, 's': 1, 'no': 1, 'problem': 1, ',': 2, 'honestly': 1, '.': 3, 'go': 2, 'on': 1, 'and': 1, 'open': 1, 'the': 1, 'launderette': 1, 'leave': 1, 'with': 1, 'me': 1}, 'female') ({'alright': 1, '.': 2, 'i': 1, \"'\": 1, 'll': 1, 'try': 1, 'and': 1, 'get': 1, 'the': 1, 'time': 1, 'off': 1, 'work': 1}, 'male')\n",
      "10113 8090 2023 5859\n"
     ]
    }
   ],
   "source": [
    "# print the number of training samples and the number of features after the split\n",
    "print(training_set[0], trainData[1], devData[0])\n",
    "print(len(training_set), len(trainData), len(devData), len(featureDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n"
     ]
    }
   ],
   "source": [
    "classifier = trainClassifier(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training!\n",
      "Precision 0.570568\n",
      "Recall: 0.570568\n",
      "F Score:0.569950\n"
     ]
    }
   ],
   "source": [
    "# testing on dev set\n",
    "results = []\n",
    "\n",
    "testTrue = [t[1] for t in devData]\n",
    "testPred = predictLabels(devData, classifier)\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "results.append((precision, recall, fscore)) # append results obtained for each training set \n",
    "results = (np.mean(np.array(results), axis = 0)) # average the cv results for precision, recall and fscore\n",
    "   # print(cv_results)\n",
    "    \n",
    "print(\"Done training!\")\n",
    "print(\"Precision %f\\nRecall: %f\\nF Score:%f\" % (results[0], results[:1], results[2]))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_trainData = []\n",
    "testData = []\n",
    "\n",
    "\n",
    "for (speech, _, gender) in training_set:\n",
    "    full_trainData.append((toFeatureVector(preProcess(speech)), gender))\n",
    "\n",
    "for (speech, _, gender) in testing_set:\n",
    "    testData.append((toFeatureVector(preProcess(speech)), gender))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n"
     ]
    }
   ],
   "source": [
    "classifier = trainClassifier(full_trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training!\n",
      "Precision 0.600344\n",
      "Recall: 0.600344\n",
      "F Score:0.599151\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "testTrue = [t[1] for t in testData]\n",
    "testPred = predictLabels(testData, classifier)\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "results.append((precision, recall, fscore)) # append results obtained for each training set \n",
    "results = (np.mean(np.array(results), axis = 0)) # average the cv results for precision, recall and fscore\n",
    "   # print(cv_results)\n",
    "    \n",
    "print(\"Done training!\")\n",
    "print(\"Precision %f\\nRecall: %f\\nF Score:%f\" % (results[0], results[:1], results[2]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've established a baseline accuracy, we will implement different preprocessing and feature engineering strategies to seek to better this accuracy score. For now we won't touch the classifier itself. \n",
    "\n",
    "Pre-processing strategies we will implement:\n",
    "\n",
    "- lower case\n",
    "- removal of white space\n",
    "- removal of stopwords\n",
    "- lemmatization\n",
    "\n",
    "Feature engineering strategies we will implement:\n",
    "\n",
    "- POS tagging\n",
    "- Named Entity Recognition\n",
    "- Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a string of one  review\n",
    "def preProcess(text, tagger=None):\n",
    "    \n",
    "    # word tokenisation, including punctuation removal\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # lowercasing\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    \n",
    "    # stopword removal- benefits are it removes rare words, though bad for bigram relations\n",
    "    stop = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop]\n",
    "        \n",
    "    # lemmatisation\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    tokens = [lemmatiser.lemmatize(t) for t in tokens]\n",
    "         \n",
    "        \n",
    "    tokens = [t for t in tokens if t] # ensure no empty space\n",
    "    \n",
    "    # if there is no tagger, we just return the unmodified sentence. \n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDict = {} # A global dictionary of features\n",
    "\n",
    "def toFeatureVector(tokens):\n",
    "\n",
    "    featureVec = {}\n",
    "\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            featureVec[w] += 1.0/len(tokens)\n",
    "        except KeyError:\n",
    "            featureVec[w] = 1.0/len(tokens)\n",
    "        try:\n",
    "            featureDict[w] += 1.0/len(tokens)\n",
    "        except KeyError:\n",
    "            featureDict[w] = 1.0/len(tokens)\n",
    "            \n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    pos_tags = [t[1] for t in tagged_tokens]\n",
    "    tag_set = set(pos_tags)\n",
    "    tag_dict = {k : pos_tags.count(k)/len(pos_tags) for k in tag_set}\n",
    "    \n",
    "    featureVec.update(tag_dict)\n",
    "    \n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add Named Entity Recognition features to the feature vector, run the 2nd SplitData and the NamedEntity function. If not run the 1st SplitData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def NamedEntity(text):\n",
    "    doc = named_entity(text)\n",
    "    named_ents = [(X.text, X.label_) for X in doc.ents]\n",
    "   \n",
    "    ents_tags = [t[1] for t in named_ents]\n",
    "    ents_set = set(ents_tags)\n",
    "    ents_dict = {k : ents_tags.count(k)/len(ents_tags) for k in ents_set}\n",
    "    \n",
    "    return ents_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - SplitData function if not using NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(training_set, percentage):\n",
    "    \n",
    "    \n",
    "    dataSamples = len(training_set) #lenght of the dataset\n",
    "    halfOfData = int(len(training_set)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2) # size of training set based on percentage chosen by user. \n",
    "    \n",
    "    for (speech, _, gender) in training_set[:trainingSamples] + training_set[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(speech)), gender))\n",
    "        \n",
    "    for (speech, _, gender) in training_set[trainingSamples:halfOfData] + training_set[halfOfData+trainingSamples:]:\n",
    "        devData.append((toFeatureVector(preProcess(speech)), gender))\n",
    "        \n",
    "    return trainData, devData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - SplitData function if using NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(training_set, percentage):\n",
    "    \n",
    "\n",
    "    dataSamples = len(training_set) #lenght of the dataset\n",
    "    halfOfData = int(len(training_set)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2) # size of training set based on percentage chosen by user. \n",
    "    \n",
    "    for (speech, _, gender) in training_set[:trainingSamples] + training_set[halfOfData:halfOfData+trainingSamples]:\n",
    "        mydict = toFeatureVector(preProcess(speech))\n",
    "        mydict.update(NamedEntity(speech))\n",
    "        trainData.append((mydict,gender))\n",
    "        \n",
    "    for (speech, _, gender) in training_set[trainingSamples:halfOfData] + training_set[halfOfData+trainingSamples:]:\n",
    "        mydict = toFeatureVector(preProcess(speech))\n",
    "        mydict.update(NamedEntity(speech))\n",
    "        devData.append((mydict,gender))\n",
    "        \n",
    "    return trainData, devData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Polarity, run 3rd SplitData and Polarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Polarity(text):\n",
    "    \n",
    "    POL = {}\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    score = sid.polarity_scores(text)['compound']\n",
    "   \n",
    "    POL = {'POL': score}\n",
    "    \n",
    "    return POL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - SplitData for use with Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(training_set, percentage):\n",
    "    \n",
    "    # A method to split the data between trainData and testData \n",
    "\n",
    "    dataSamples = len(training_set) #lenght of the dataset\n",
    "    halfOfData = int(len(training_set)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2) # size of training set based on percentage chosen by user. \n",
    "    \n",
    "    for (speech, _, gender) in training_set[:trainingSamples] + training_set[halfOfData:halfOfData+trainingSamples]:\n",
    "        mydict = toFeatureVector(preProcess(speech))\n",
    "        mydict.update(Polarity(speech))\n",
    "        trainData.append((mydict,gender))\n",
    "        \n",
    "    for (speech, _, gender) in training_set[trainingSamples:halfOfData] + training_set[halfOfData+trainingSamples:]:\n",
    "        mydict = toFeatureVector(preProcess(speech))\n",
    "        mydict.update(Polarity(speech))\n",
    "        devData.append((mydict,gender))\n",
    "        \n",
    "    return trainData, devData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading reviews\n",
    "# initialize global lists that will be appended to by the methods below\n",
    "#rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "trainData = []        # the pre-processed training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "devData = []         # the pre-processed test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "#random.seed(4)\n",
    "#shuffle(training_set)\n",
    "trainData, devData = splitData(training_set, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Someone had fun.', 'SEAN', 'male') ({'last': 0.16666666666666666, 'night': 0.16666666666666666, 'better': 0.16666666666666666, 'ever': 0.16666666666666666, 'anything': 0.16666666666666666, 'interesting': 0.16666666666666666, 'RB': 0.16666666666666666, 'RBR': 0.16666666666666666, 'JJ': 0.3333333333333333, 'NN': 0.3333333333333333}, 'male') ({'alright': 0.2, 'try': 0.2, 'get': 0.2, 'time': 0.2, 'work': 0.2, 'NN': 0.4, 'JJ': 0.2, 'VB': 0.4}, 'male')\n",
      "10113 8090 2023 5428\n"
     ]
    }
   ],
   "source": [
    "# We print the number of training samples and the number of features after the split\n",
    "print(training_set[0], trainData[2], devData[0])\n",
    "print(len(training_set), len(trainData), len(devData), len(featureDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(C = 0.1, max_iter=6500))])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n"
     ]
    }
   ],
   "source": [
    "classifier = trainClassifier(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training!\n",
      "Precision 0.563617\n",
      "Recall: 0.563617\n",
      "F Score:0.563482\n"
     ]
    }
   ],
   "source": [
    "# testing on dev set\n",
    "results = []\n",
    "\n",
    "testTrue = [t[1] for t in devData]\n",
    "testPred = predictLabels(devData, classifier)\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "results.append((precision, recall, fscore)) # append results obtained for each training set \n",
    "results = (np.mean(np.array(results), axis = 0)) # average the cv results for precision, recall and fscore\n",
    "   # print(cv_results)\n",
    "    \n",
    "print(\"Done training!\")\n",
    "print(\"Precision %f\\nRecall: %f\\nF Score:%f\" % (results[0], results[:1], results[2]))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will add tf-idf to the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('tfidf', TfidfTransformer()), ('svc', LinearSVC(C = 0.1, max_iter=4500))])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n"
     ]
    }
   ],
   "source": [
    "classifier = trainClassifier(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training!\n",
      "Precision 0.569441\n",
      "Recall: 0.569441\n",
      "F Score:0.569443\n"
     ]
    }
   ],
   "source": [
    "# testing on dev set\n",
    "results = []\n",
    "\n",
    "testTrue = [t[1] for t in devData]\n",
    "testPred = predictLabels(devData, classifier)\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "results.append((precision, recall, fscore)) # append results obtained for each training set \n",
    "results = (np.mean(np.array(results), axis = 0)) # average the cv results for precision, recall and fscore\n",
    "   # print(cv_results)\n",
    "    \n",
    "print(\"Done training!\")\n",
    "print(\"Precision %f\\nRecall: %f\\nF Score:%f\" % (results[0], results[:1], results[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we indicate the accuracy scores for different combinations of preprocess and features. The highest score here is Stop Word Removal, with POS tags and TD-IDF. The second highest is Stop Word Removal, with POS tags, Polarity and TD-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dev set accuracy scores**\n",
    "\n",
    "Lemmatization, POS Tag:\n",
    "    \n",
    "\n",
    "- Precision 0.536866\n",
    "- Recall: 0.536866\n",
    "- F Score:0.535487\n",
    "\n",
    "Lemmatization, NER:\n",
    "\n",
    "- Precision 0.537645\n",
    "- Recall: 0.537645\n",
    "- F Score:0.536910\n",
    "\n",
    "SWR, NER:\n",
    "\n",
    "\n",
    "- Precision 0.551256\n",
    "- Recall: 0.551256\n",
    "- F Score:0.549940\n",
    "    \n",
    "SWR, POS tag:\n",
    "\n",
    "\n",
    "- Precision 0.563617\n",
    "- Recall: 0.563617\n",
    "- F Score:0.563482\n",
    "\n",
    "\n",
    "SWR, Lemmatization, POS tag:\n",
    "\n",
    "\n",
    "- Precision 0.551689\n",
    "- Recall: 0.551689\n",
    "- F Score:0.551656\n",
    "\n",
    "SWR, Lemmatization, NER:\n",
    "\n",
    "    \n",
    "- Precision 0.542277\n",
    "- Recall: 0.542277\n",
    "- F Score:0.541064\n",
    "\n",
    "SWR, Lemmatization, POS tag, NER:\n",
    "\n",
    "\n",
    "- Precision 0.545692\n",
    "- Recall: 0.545692\n",
    "- F Score:0.545637\n",
    "\n",
    "SWR, Lemmatization, POS tag, NER, TF-IDF:\n",
    "\n",
    "\n",
    "- Precision 0.553608\n",
    "- Recall: 0.553608\n",
    "- F Score:0.553507\n",
    "\n",
    "SWR, Lemmatization, POS tag, TF-IDF:\n",
    "\n",
    "\n",
    "- Precision 0.561530\n",
    "- Recall: 0.561530\n",
    "- F Score:0.561387\n",
    "\n",
    "SWR, POS tag, Polarity, TF-IDF:\n",
    "\n",
    "- Precision 0.566971\n",
    "- Recall: 0.566971\n",
    "- F Score:0.566973\n",
    "\n",
    "SWR, POS tag, TF-IDF:\n",
    "\n",
    "\n",
    "- Precision 0.569441\n",
    "- Recall: 0.569441\n",
    "- F Score:0.569443\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a pipeline and perform gridsearch using several classifiers and parameters to examine how this might affect our results.  We'll pass our feature dictionary to matricial form for ease of use and check the following classifiers:\n",
    "\n",
    "- SVC\n",
    "- SGD\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [t[0] for t in trainData]\n",
    "Y_train = [t[1] for t in trainData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict_Vectorizer = DictVectorizer()\n",
    "X_train_mat = Dict_Vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters = [{'model__kernel': ['rbf'], 'model__gamma': [1e-3, 1e-4],\n",
    "                     'model__C': [0.1, 1, 10, 100]},\n",
    "              {'model__kernel': ['linear'], 'model__C': [0.1, 1, 10, 100]}\n",
    "                    ]\n",
    "    \n",
    "pipeline =  Pipeline([('tfidf', TfidfTransformer()), ('model', SVC())])\n",
    "    \n",
    "\n",
    "grid_search = GridSearchCV(pipeline, cv=5, param_grid= parameters, scoring= 'roc_auc')\n",
    "    \n",
    "grid_search.fit(X_train_mat, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.590\n",
      "Best parameters set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model__C': 1, 'model__kernel': 'linear'}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('tfidf', TfidfTransformer()),\n",
       "                                       ('model', SGDClassifier())]),\n",
       "             param_grid=[{'model__alpha': (1e-07, 1e-06, 1e-05, 0.0001, 0.001,\n",
       "                                           0.01, 0.1, 1.0),\n",
       "                          'model__penalty': ('l2', 'elasticnet', 'l1')}],\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = [\n",
    "            {'model__alpha': (1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0),\n",
    "            'model__penalty': ('l2', 'elasticnet', 'l1')}]\n",
    "\n",
    "pipeline =  Pipeline([('tfidf', TfidfTransformer()), ('model', SGDClassifier())])\n",
    "    \n",
    "grid_search = GridSearchCV(pipeline, cv=10, param_grid= parameters, scoring= 'roc_auc')\n",
    "    \n",
    "grid_search.fit(X_train_mat, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.591\n",
      "Best parameters set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model__alpha': 0.0001, 'model__penalty': 'elasticnet'}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = [{\n",
    " 'model__max_depth': [2000],\n",
    " 'model__min_samples_split': [100],\n",
    " 'model__max_leaf_nodes': [None]\n",
    "}]\n",
    "\n",
    "pipeline = Pipeline([('tfidf',TfidfTransformer()), ('model', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfidf', TfidfTransformer()),\n",
       "                                       ('model', RandomForestClassifier())]),\n",
       "             param_grid=[{'model__max_depth': [2000],\n",
       "                          'model__max_leaf_nodes': [None],\n",
       "                          'model__min_samples_split': [100]}],\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(pipeline, param_grid= random_grid, cv = 5, scoring= 'roc_auc')\n",
    "grid_search.fit(X_train_mat, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.589\n",
      "Best parameters set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model__max_depth': 2000,\n",
       " 'model__max_leaf_nodes': None,\n",
       " 'model__min_samples_split': 100}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results obtained using grid search and cross validation indicate that the choice between these classifiers does not change the accuracy outcome. Because of that we can use SGD which showed a fractionally higher accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the features and classifiers on the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test the two highest scoring combination of pre-processing and features obtained on the development set on the full dataset and test on the testing set. We will use SGD as our classifier. \n",
    "\n",
    "- SWR, POS tag, Polarity, TF-IDF\n",
    "- SWR, POS tag, TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a string of one  review\n",
    "def preProcess(text, tagger=None):\n",
    "    \n",
    "    # word tokenisation, including punctuation removal\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # lowercasing\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    \n",
    "    # stopword removal- benefits are it removes rare words, though bad for bigram relations\n",
    "    stop = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop]\n",
    "         \n",
    "        \n",
    "    tokens = [t for t in tokens if t] # ensure no empty space\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDict = {} # A global dictionary of features\n",
    "\n",
    "def toFeatureVector(tokens):\n",
    "\n",
    "    featureVec = {}\n",
    "\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            featureVec[w] += 1.0/len(tokens)\n",
    "        except KeyError:\n",
    "            featureVec[w] = 1.0/len(tokens)\n",
    "        try:\n",
    "            featureDict[w] += 1.0/len(tokens)\n",
    "        except KeyError:\n",
    "            featureDict[w] = 1.0/len(tokens)\n",
    "            \n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    pos_tags = [t[1] for t in tagged_tokens]\n",
    "    tag_set = set(pos_tags)\n",
    "    tag_dict = {k : pos_tags.count(k)/len(pos_tags) for k in tag_set}\n",
    "    \n",
    "    featureVec.update(tag_dict)\n",
    "    \n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the data with no added polarity values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_trainData = []\n",
    "testData = []\n",
    "\n",
    "\n",
    "for (speech, _, gender) in training_set:\n",
    "    full_trainData.append((toFeatureVector(preProcess(speech)), gender))\n",
    "\n",
    "for (speech, _, gender) in testing_set:\n",
    "    testData.append((toFeatureVector(preProcess(speech)), gender))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the data if we want to use polarity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_trainData = []\n",
    "testData = []\n",
    "\n",
    "\n",
    "for (speech, _, gender) in training_set:\n",
    "    mydict = toFeatureVector(preProcess(speech))\n",
    "    mydict.update(Polarity(speech))\n",
    "    full_trainData.append((mydict, gender))\n",
    "\n",
    "for (speech, _, gender) in testing_set:\n",
    "    mydict = toFeatureVector(preProcess(speech))\n",
    "    mydict.update(Polarity(speech))\n",
    "    testData.append((mydict, gender))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'last': 0.16666666666666666, 'night': 0.16666666666666666, 'better': 0.16666666666666666, 'ever': 0.16666666666666666, 'anything': 0.16666666666666666, 'interesting': 0.16666666666666666, 'RB': 0.16666666666666666, 'RBR': 0.16666666666666666, 'JJ': 0.3333333333333333, 'NN': 0.3333333333333333, 'POL': 0.7149}, 'male') ({'hang': 0.3333333333333333, 'pellets': 0.3333333333333333, 'cupboard': 0.3333333333333333, 'NNS': 0.3333333333333333, 'VBP': 0.3333333333333333, 'NN': 0.3333333333333333, 'POL': 0.0}, 'female')\n"
     ]
    }
   ],
   "source": [
    "print(full_trainData[2], testData[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('tfidf', TfidfTransformer()), ('SDG', SGDClassifier(alpha= 0.0001, penalty= 'elasticnet', max_iter= 3000, random_state= 4))])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n"
     ]
    }
   ],
   "source": [
    "classifier = trainClassifier(full_trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training!\n",
      "Precision 0.587784\n",
      "Recall: 0.587784\n",
      "F Score:0.584851\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "testTrue = [t[1] for t in testData]\n",
    "testPred = predictLabels(testData, classifier)\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "results.append((precision, recall, fscore)) # append results obtained for each training set \n",
    "results = (np.mean(np.array(results), axis = 0)) # average the cv results for precision, recall and fscore\n",
    "   # print(cv_results)\n",
    "    \n",
    "print(\"Done training!\")\n",
    "print(\"Precision %f\\nRecall: %f\\nF Score:%f\" % (results[0], results[:1], results[2]))   \n",
    "# QUESTION 3 - Make sure there is a function call here to the\n",
    "# crossValidate function on the training set to get your results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for SWR, POS tag, TF-IDF:\n",
    "\n",
    "- Precision 0.576786\n",
    "- Recall: 0.576786\n",
    "- F Score:0.574224\n",
    "\n",
    "Results for SWR, POS tag, Polarity, TF-IDF:\n",
    "\n",
    "- Precision 0.587784\n",
    "- Recall: 0.587784\n",
    "- F Score:0.584851"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination SWR, POS tag, Polarity, TF-IDF despite scoring slightly lower on the dev set, scored higher on the full testing set, with an f-score accuracy value of 0.585. Surprisingly however the very simplest of model run as baseline continues to outperform the complex model above, with an f-score accuracy value of 0.599. This makes us wonder if there is an error in our process, but if so it isn't immediately apparent to us. This being so, strictly speaking we would have to choose the simpler model over the more complex one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
