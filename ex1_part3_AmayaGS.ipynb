{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignement 1 - Q5\n",
    "\n",
    "Amaya Syed - 190805496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "import nltk\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue improving accuracy the next step is to add some of the other features available in the corpus. First we will load the data and take a look at the features available. We will also check, at a qualitative level, if there are any obvious class imbalances which might guide us in selecting appropriate features for the classifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOC_ID</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>RATING</th>\n",
       "      <th>VERIFIED_PURCHASE</th>\n",
       "      <th>PRODUCT_CATEGORY</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>PRODUCT_TITLE</th>\n",
       "      <th>REVIEW_TITLE</th>\n",
       "      <th>REVIEW_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>PC</td>\n",
       "      <td>B00008NG7N</td>\n",
       "      <td>Targus PAUK10U Ultra Mini USB Keypad, Black</td>\n",
       "      <td>useful</td>\n",
       "      <td>When least you think so, this product will sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "      <td>Wireless</td>\n",
       "      <td>B00LH0Y3NM</td>\n",
       "      <td>Note 3 Battery : Stalion Strength Replacement ...</td>\n",
       "      <td>New era for batteries</td>\n",
       "      <td>Lithium batteries are something new introduced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>3</td>\n",
       "      <td>N</td>\n",
       "      <td>Baby</td>\n",
       "      <td>B000I5UZ1Q</td>\n",
       "      <td>Fisher-Price Papasan Cradle Swing, Starlight</td>\n",
       "      <td>doesn't swing very well.</td>\n",
       "      <td>I purchased this swing for my baby. She is 6 m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>B003822IRA</td>\n",
       "      <td>Casio MS-80B Standard Function Desktop Calculator</td>\n",
       "      <td>Great computing!</td>\n",
       "      <td>I was looking for an inexpensive desk calcolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>B00PWSAXAM</td>\n",
       "      <td>Shine Whitening - Zero Peroxide Teeth Whitenin...</td>\n",
       "      <td>Only use twice a week</td>\n",
       "      <td>I only use it twice a week and the results are...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DOC_ID       LABEL  RATING VERIFIED_PURCHASE PRODUCT_CATEGORY  PRODUCT_ID  \\\n",
       "0       1  __label1__       4                 N               PC  B00008NG7N   \n",
       "1       2  __label1__       4                 Y         Wireless  B00LH0Y3NM   \n",
       "2       3  __label1__       3                 N             Baby  B000I5UZ1Q   \n",
       "3       4  __label1__       4                 N  Office Products  B003822IRA   \n",
       "4       5  __label1__       4                 N           Beauty  B00PWSAXAM   \n",
       "\n",
       "                                       PRODUCT_TITLE  \\\n",
       "0        Targus PAUK10U Ultra Mini USB Keypad, Black   \n",
       "1  Note 3 Battery : Stalion Strength Replacement ...   \n",
       "2       Fisher-Price Papasan Cradle Swing, Starlight   \n",
       "3  Casio MS-80B Standard Function Desktop Calculator   \n",
       "4  Shine Whitening - Zero Peroxide Teeth Whitenin...   \n",
       "\n",
       "               REVIEW_TITLE                                        REVIEW_TEXT  \n",
       "0                    useful  When least you think so, this product will sav...  \n",
       "1     New era for batteries  Lithium batteries are something new introduced...  \n",
       "2  doesn't swing very well.  I purchased this swing for my baby. She is 6 m...  \n",
       "3          Great computing!  I was looking for an inexpensive desk calcolat...  \n",
       "4     Only use twice a week  I only use it twice a week and the results are...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading data using pandas and taking a look at the first few lines\n",
    "data = pd.read_csv(\"amazon_reviews.txt\", delimiter = \"\\t\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LABEL\n",
       "__label1__    316.550000\n",
       "__label2__    428.102857\n",
       "Name: review_length, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for inbalances in the dataset for fake or real reviews, in procduct category, rating, verified purchase and review length\n",
    "label_prod_counts = data.groupby(data[\"LABEL\"]).PRODUCT_CATEGORY.value_counts()\n",
    "label_rating_counts = data.groupby(data[\"LABEL\"]).RATING.value_counts()\n",
    "rating_label_counts = data.groupby(data[\"RATING\"]).LABEL.value_counts()\n",
    "verified_purchase_label_counts = data.groupby(data[\"LABEL\"]).VERIFIED_PURCHASE.value_counts()\n",
    "rating_vp_counts = data.groupby(data[\"RATING\"]).VERIFIED_PURCHASE.value_counts()\n",
    "\n",
    "data['review_length'] = data['REVIEW_TEXT'].apply(len)\n",
    "data.groupby([\"LABEL\"]).review_length.mean()\n",
    "\n",
    "# label_prod_counts - 350 reviews in each product category. \n",
    "# label_rating_counts - roughly same number of reviews for each rating \n",
    "# verified_purchase_label_counts - WAY MORE real reviews in verified purchase. \n",
    "# review_length is slighlty longer for fake reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imbalance in the verified purchase vs label is the only one to really jump out here, although it seems the review length is slightly longer in fake reviews, which makes sense if one considers the spontaneous nature of a real review as compared to a planned fake review. We will therefore concentrate on adding as features whether the review was made after a *Verified Purchase*. We will also add the review length and finally, although we have seen there is a balanced number of fake and real reviews for the different ratings, we will also add *Rating* as a category - as one would intuitively expect there might be a difference between reviews based on their ratings if they are real or fake. To add these features we will modify the **paseReview**, **toFeatureVector**, **loadData** and **splitData** slightly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **paseReview** review we will now keep the elements 2 and 3 of the input list, corresponding to the Rating and Verified purchase status of the product reviewed. We then return (Id, Text, Label, Rating, Verified_purchase) and assign these to new variables within **loadData**, as well as append them to *rawData*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_int(element):\n",
    "    \n",
    "    \"\"\"\n",
    "    Auxiliary function which takes an element of a list and tries to pass it from string to integer. If it can't it returns the original element.'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        element = int(element) # trying to convert string to integer\n",
    "    except ValueError: # if Value Error is raised because the string can't be passed to integer, then pass and return original input\n",
    "        pass\n",
    "    return element\n",
    "\n",
    "\n",
    "def parseReview(line):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a triple of an integer, a string containing the review, and a string indicating the label.\n",
    "  \n",
    "    \"\"\"\n",
    "    indices = [0, 1, 2, 3, 8] # here we add Rating and Verified purchase in addition to id, text and label\n",
    "    label_dict = {'__label1__':'fake', '__label2__':'real'} # creating a dictionary assigning labels to more descriptive terms. \n",
    "            \n",
    "    review = [line[index] for index in indices] # 2) keep id, label, text using the indices\n",
    "    review = [label_dict.get(x, x) for x in review] # 3) passing the label names to real or fake\n",
    "    review = [str_to_int(x) for x in review] # 4) pass id to int using the conv function defined above\n",
    "    Id, Label, Rating, Verified_purchase, Text = review # 5) assign each element of the list to a separate variable\n",
    "        \n",
    "    return Id, Text, Label, Rating, Verified_purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) TEXT PREPROCESSING AND FEATURE VECTORIZATION        \n",
    "\n",
    "# Input: a string of one  review\n",
    "def preProcess(text):\n",
    "    \n",
    "    tokens = []\n",
    "    output = []  \n",
    "    \n",
    "    for token in text.split():\n",
    "        \n",
    "        tokens.append(token.lower())\n",
    "        \n",
    "        output = [' '.join(token) for token in nltk.bigrams(tokens)] + tokens\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference in the code is in **toFeatureVector**, as we need to modify the dictionary in such as way that we now also have keys which reflect the verified purchase status and the rating assigned to the product. To do so we simply input *Rating* and *Verified_purchase* as parameters of the function and create keys for them within our local dictionary *token_frequency*. We then return the dictionary with these new entry keys for each review. The function **splitData** is modified slightly to add the new parameters inputed into **toFeatureVector** and then to append the *trainData* and *testData* properly. After having done that we simply train our model as before. We see the testing accuracy obtained at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDict = {}\n",
    "\n",
    "def toFeatureVector(tokens, Rating, Verified_purchase):\n",
    "    \n",
    "    token_frequency = {} # creating a dictionary to store token frequency\n",
    "\n",
    "# Rating\n",
    " \n",
    "    token_frequency[\"Rating\"] = Rating # creating a key to store the rating given to the product\n",
    "\n",
    "# Verified_Purchase\n",
    "\n",
    "    if Verified_purchase == \"N\":  # creating a key for Verified purchase and storing N as 0 and Y as 1\n",
    "        token_frequency[\"Verified Purchase\"] = 0\n",
    "    else:\n",
    "        token_frequency[\"Verified Purchase\"] = 1\n",
    "        \n",
    "# length review\n",
    "\n",
    "    token_frequency['Length Review'] = len(tokens) # creating a key for the number of tokens in the review. This counts bigrams as well. \n",
    "\n",
    "# Text is counted as it was before\n",
    "\n",
    "    for token in tokens: # for each word in the review\n",
    "    \n",
    "        if token not in token_frequency.keys(): \n",
    "            token_frequency[token] = 1  # if the word is not within the dic we add it as key = 1\n",
    "        else:\n",
    "            token_frequency[token] += 1  # if its already in the dic, we just add one\n",
    "        \n",
    "        if token not in featureDict.keys():\n",
    "            featureDict[token] = 1\n",
    "        else:\n",
    "            featureDict[token] += 1\n",
    "        \n",
    "    return token_frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path, Text=None):\n",
    "    \n",
    "    with open(path, encoding='utf8') as f:\n",
    "        \n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        next(reader)\n",
    "        \n",
    "        for line in reader:\n",
    "            \n",
    "            (Id, Text, Label, Rating, Verified_purchase) = parseReview(line)\n",
    "            \n",
    "            rawData.append((Id, Text, Label, Rating, Verified_purchase))\n",
    "          \n",
    "def splitData(percentage):\n",
    "    \n",
    "    # A method to split the data between trainData and testData \n",
    "    \n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    \n",
    "    for (_, Text, Label, Rating, Verified_purchase) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Text), Rating, Verified_purchase), Label))\n",
    "        \n",
    "    for (_, Text, Label, Rating, Verified_purchase) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Text), Rating, Verified_purchase), Label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidate(trainData, folds):\n",
    "\n",
    "    shuffle(trainData) # reshuffle dataset as currently its ordered into fake, then real reviews\n",
    "    cv_results = [] # initiate list to keep cv results for each fold\n",
    "    foldSize = int(len(trainData) / folds) # initiate fold size by dividing training data by # folds.\n",
    "\n",
    "    # DESCRIBE YOUR METHOD IN WORD\n",
    "    \n",
    "    for i in range(0, len(trainData), foldSize): # data goes from i to lenght of data in foldsize jumps.\n",
    "        \n",
    "        cv_test = trainData[i : i + foldSize]\n",
    "        cv_train  = trainData[0 : i] + trainData[i + foldSize : ]\n",
    "        \n",
    "        classifier = trainClassifier(cv_train)  # train the classifier\n",
    "        testTrue = [t[1] for t in cv_test]   # get the ground-truth labels from the data\n",
    "        testPred = predictLabels(cv_test, classifier)  # classify the test data to get predicted labels\n",
    "        precision, recall, fscore, _ = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "        cv_results.append((precision, recall, fscore))\n",
    "    cv_results = (np.mean(np.array(cv_results), axis = 0))\n",
    "   # print(cv_results)\n",
    "    print(\"Done training!\")\n",
    "    print(\"Precision %f\\nRecall: %f\\nF Score:%f\" % (cv_results[0], cv_results[:1], cv_results[2]))   \n",
    "        \n",
    "    \n",
    "    return cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(C=0.001, max_iter = 3500))])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 21000 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "After split, 21000 rawData, 16800 trainData, 4200 testData\n",
      "Training Samples: \n",
      "16800\n",
      "Features: \n",
      "592379\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Done training!\n",
      "Precision 0.815087\n",
      "Recall: 0.815087\n",
      "F Score:0.810187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.81508737, 0.81083333, 0.81018722])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "folds = 10\n",
    "# loading reviews\n",
    "# initialize global lists that will be appended to by the methods below\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "trainData = []        # the pre-processed training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the pre-processed test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "# the output classes\n",
    "fakeLabel = 'fake'\n",
    "realLabel = 'real'\n",
    "\n",
    "# references to the data files\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "# Do the actual stuff (i.e. call the functions we've made)\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "\n",
    "loadData(reviewPath) \n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# You do the cross validation on the 80% (training data)\n",
    "# We print the number of training samples and the number of features before the split\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "\n",
    "splitData(0.8)\n",
    "\n",
    "# We print the number of training samples and the number of features after the split\n",
    "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')\n",
    "\n",
    "# QUESTION 3 - Make sure there is a function call here to the\n",
    "# crossValidate function on the training set to get your results\n",
    "\n",
    "crossValidate(trainData, folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'Rating': 5, 'Verified Purchase': 0, 'Length Review': 41, 'this assortment': 1, 'assortment is': 1, 'is really': 1, \"really hershey's\": 1, \"hershey's at\": 1, 'at their': 1, 'their best.': 1, 'best. the': 1, 'the little': 1, 'little ones': 1, 'ones are': 1, 'are always': 1, 'always excited': 1, 'excited whenever': 1, 'whenever the': 1, 'the holidays': 1, 'holidays come': 1, 'come because': 1, 'because of': 1, 'of this.': 1, 'this': 1, 'assortment': 1, 'is': 1, 'really': 1, \"hershey's\": 1, 'at': 1, 'their': 1, 'best.': 1, 'the': 2, 'little': 1, 'ones': 1, 'are': 1, 'always': 1, 'excited': 1, 'whenever': 1, 'holidays': 1, 'come': 1, 'because': 1, 'of': 1, 'this.': 1}, 'fake')\n",
      "Training Classifier...\n",
      "Done training!\n",
      "Precision: 0.823328\n",
      "Recall: 0.818333\n",
      "F Score:0.817629\n"
     ]
    }
   ],
   "source": [
    "# # Evaluate on test set\n",
    "\n",
    "# Finally, check the accuracy of your classifier by training on all the tranin data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(testData[0])   # have a look at the first test data instance\n",
    "    classifier = trainClassifier(trainData)  # train the classifier\n",
    "    testTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
    "    testPred = predictLabels(testData, classifier)  #Â classify the test data to get predicted labels\n",
    "    finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "    print(\"Done training!\")\n",
    "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % finalScores[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the feature set**\n",
    "\n",
    "- Review text, verified purchase:\n",
    "\n",
    "\n",
    "    - Precision: 0.822767\n",
    "    - Recall: 0.817857\n",
    "    - F Score:0.817162\n",
    "\n",
    "- Review text, rating:\n",
    "\n",
    "\n",
    "    - Precision: 0.606135\n",
    "    - Recall: 0.605952\n",
    "    - F Score:0.605783\n",
    "\n",
    "- Review text, verified purchase, rating:\n",
    "\n",
    "\n",
    "    - Precision: 0.822487\n",
    "    - Recall: 0.817619\n",
    "    - F Score:0.816928\n",
    "\n",
    "- Review text, review lenght:\n",
    "\n",
    "\n",
    "    - Precision: 0.635665\n",
    "    - Recall: 0.635000\n",
    "    - F Score:0.634552\n",
    "\n",
    "- Review text, verified purchase, review lenght:\n",
    "\n",
    "\n",
    "    - Precision: 0.822922\n",
    "    - Recall: 0.817857\n",
    "    - F Score:0.817140\n",
    "\n",
    "- Review text, rating, verified purchase and review lenght:\n",
    "\n",
    "\n",
    "    - Precision: 0.823328\n",
    "    - Recall: 0.818333\n",
    "    - F Score:0.817629"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the combination of vectors that the feature which most contributes to the increasing testing accuracy is *Verified purchase*, which increases our Precision score from 0.63 to to 0.82 - a nearly 20 points improvement reached by only adding one feature. Clearly this feature is given a lot of importance when the specialists at amazon manually curate the reviews and decide which are real and which are fake. Regarding other features, review length alone only gave us an increase of 1 point in overall accuracy, whilst rating alone actually resulted in decrease accuracy. The combination of these three features gave a very marginal improvement in accuracy compared to using *Verified purchase* alone - reaching scores of **Precision: 0.823, Recall: 0.818, F-Score:0.818**. \n",
    "\n",
    "As explained in part 2 of this assignment, to further improve this score, some kind of weighting scheme for the tokens in the *token_frequency* dictionary would be an important avenue to explore, but a good strategy to do this wasn't found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
